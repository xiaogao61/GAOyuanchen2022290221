import pandas as pd
import jieba
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from model_ollama import query_model  # ä½¿ç”¨ä½ å·²æœ‰çš„æœ¬åœ°å¤§æ¨¡å‹æ¥å£


# âœ… è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºï¼ˆè§£å†³çƒ­åŠ›å›¾åæ ‡ä¹±ç ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# 1. åŠ è½½æ•°æ®
df = pd.read_csv("train.csv").sample(n=10, random_state=42)
texts = df["content"].astype(str).tolist()

# 2. ä¸­æ–‡åˆ†è¯ä¸æ¸…æ´—
def preprocess(text):
    text = re.sub(r"[^\u4e00-\u9fa5]", "", text)  # ä¿ç•™ä¸­æ–‡
    tokens = jieba.cut(text)
    return " ".join([t for t in tokens if len(t) > 1])

texts_cleaned = [preprocess(text) for text in texts]

# 3. æ–‡æœ¬å‘é‡åŒ–ï¼ˆè¯è¢‹æ¨¡å‹ï¼‰
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts_cleaned)

# 4. LDAå»ºæ¨¡
lda = LatentDirichletAllocation(n_components=3, random_state=42)
lda.fit(X)

# 5. æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
def print_topics(model, feature_names, n_top_words):
    for idx, topic in enumerate(model.components_):
        print(f"ä¸»é¢˜ {idx + 1}: ", " ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))

print_topics(lda, vectorizer.get_feature_names_out(), 10)

# 6. è¯äº‘å›¾å¯è§†åŒ–
for topic_idx, topic in enumerate(lda.components_):
    freqs = {vectorizer.get_feature_names_out()[i]: topic[i] for i in topic.argsort()[:-30 - 1:-1]}
    wc = WordCloud(font_path='simhei.ttf', background_color='white', width=800, height=400)
    wc.generate_from_frequencies(freqs)
    plt.figure()
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"ä¸»é¢˜ {topic_idx + 1} è¯äº‘")
    plt.show()


# 8. çƒ­åŠ›å›¾ï¼šæ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
doc_topic_matrix = lda.transform(X)
df_doc_topic = pd.DataFrame(doc_topic_matrix, columns=[f"ä¸»é¢˜{i+1}" for i in range(lda.n_components)])
df_doc_topic.index = [f"å¾®åš{i+1}" for i in range(len(texts))]

plt.figure(figsize=(10, 6))
sns.heatmap(df_doc_topic, cmap="YlOrRd", annot=True)
plt.title("ğŸ“Š æ¯ç¯‡å¾®åšçš„ä¸»é¢˜æ¦‚ç‡çƒ­åŠ›å›¾")
plt.xlabel("ä¸»é¢˜")
plt.ylabel("å¾®åš")
plt.tight_layout()
plt.show()

# 9. è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹åˆ†ææ¯ä¸ªä¸»é¢˜çš„è¯­ä¹‰å«ä¹‰
feature_names = vectorizer.get_feature_names_out()
topn = 10
for topic_idx, topic in enumerate(lda.components_):
    top_words = [feature_names[i] for i in topic.argsort()[:-topn - 1:-1]]
    prompt = f"""ä»¥ä¸‹æ˜¯å¾®åšä¸»é¢˜æ¨¡å‹ä¸­ç¬¬{topic_idx + 1}ä¸ªä¸»é¢˜çš„å…³é”®è¯ï¼š
{', '.join(top_words)}

è¯·ä½ æ ¹æ®è¿™äº›å…³é”®è¯æ¨æµ‹è¿™ä¸ªä¸»é¢˜ä¸»è¦åœ¨è®¨è®ºä»€ä¹ˆå†…å®¹ï¼Ÿè¯·ç”¨ç®€æ´ä¸­æ–‡æ€»ç»“ï¼Œä¸è¶…è¿‡50å­—ã€‚"""
    result = query_model(prompt)
    print(f"\nğŸ§  ä¸»é¢˜ {topic_idx + 1} å…³é”®è¯ï¼š{top_words}")
    print(f"ğŸ¤– å¤§æ¨¡å‹åˆ†æç»“æœï¼š{result.strip()}")
